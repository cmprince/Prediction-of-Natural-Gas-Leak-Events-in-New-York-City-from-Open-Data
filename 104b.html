<div>Several models were implemented: linear regression, linear regression with Lasso (L1) and Ridge (L2) regularization. Recursive linear regression&nbsp;was also implemented on top features identified through multiple feature selection techniques. Top features were identified through multiple techniques, including  linear correlation, lasso regularization, maximal information coefficient (similar to linear coefficient, but can identify non-linear relationships), recursive feature elimination, and ridge regularization. A random forest model and a simple multi-layer perceptron (MLP) neural network were also implemented. All of the models were generated in python, using the scikit-learn machine learning packages,<cite class="ltx_cite raw v1">\cite{scikit-learn}</cite>, except the neural network which was developed using tensorflow and its python bindings.<cite class="ltx_cite raw v1">\cite{tensorflow2015-whitepaper}</cite> The codes for these models are available in the public code repository accompanying this paper.<cite class="ltx_cite raw v1">\cite{github}</cite></div><div>Each model was initially trained on 2013 data, using 2013 gas leaks as the target variable to properly weight model parameters. They were then cross-validated by predicting 2014 gas leaks using 2013 features, with the assumption that a true predictive model would have only the previous years' data to predict the next year's leaks. In this manner, each model was optimized by tuning hyper-parameters (such as specific features selected with linear regression or the learning rate and number of hidden units in the neural network). Once sufficiently optimized, each model was tested by predicting 2015 gas leaks using 2014 features and compared based on overall RMSE.</div><div></div><h1 data-label="669578" class="ltx_title_section">Results</h1><div>Tables I and II and Figures <span class="au-ref raw v1">\ref{576363}</span> and <span class="au-ref raw v1">\ref{508370}</span> give visual and tabular performance indicators for each of the models. &nbsp;At the zip code level and the census tract level, the naive model boasted the lowest overall RMSE, with 0.002446 at the zip code level and 0.3173 at the census tract level.</div><div><table data-toggle="context" data-target="#tableContextMenu" class="ltx_tabular ltx_tabular_fullpage"><caption>Top Model Performance (Zip Code Level)</caption><tbody><tr><td class="ltx_framed ltx_align_center"><b>Model</b></td> <td class="ltx_framed ltx_align_center"><b>Total</b> <b>RMSE</b></td> </tr> <tr> <td class="ltx_framed ltx_align_center">Naive</td> <td class="ltx_framed ltx_align_center">0.002446</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Random forest</td> <td class="ltx_framed ltx_align_center">0.002946</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Ridge regression</td> <td class="ltx_framed ltx_align_center">0.003661</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Linear regression (select features)</td> <td class="ltx_framed ltx_align_center">0.003663</td> </tr> <tr> <td class="ltx_framed ltx_align_center">MLP neural network</td> <td class="ltx_framed ltx_align_center">0.005156</td></tr></tbody></table></div><div></div>