<div></div><h2 data-label="207714" class="ltx_title_subsection">Methodology</h2><div>Our research idea was to investigate whether&nbsp;analytical models that are trained and calibrated on 2013-2014 data will more accurately predict 2015 gas leaks than a naive guess. Our working null hypothesis was there there would be&nbsp;no significant improvement with the given model than a naive method. We defined the naive method to be the average number of actual gas leaks (per zip code or tract) in the 2013-2014 data. This annual average would be the predicted number of gas leaks in 2015. Our target prediction value was the number of leaks per building unit (per zip code or tract). For final model evaluation, we used Root Mean Squared Error (RMSE) of the predicted leaks per building unit relative to actual. For zip code level, we compared predictions to actual number of leaks; however since that data was not available at the tract level, we compared predicted leaks to the number of leaks calculated using the road segment approach described above. RMSE is scale-dependent, so while it would not be appropriate for comparing the effects of non-scaled variables, it is appropriate for comparing model performance on the same dataset.</div><div></div><div>Several models were implemented: linear regression, linear regression with Lasso (L1) and Ridge (L1) regularization, and recursive linear regression to identify maximal information coefficient (and cross-validated). [NATE: can you elaborate on this in plain english? Not sure I can explain...]. A random forest model and a simple multi-layer perceptron neural network were also implemented.</div><div></div><div>Each model was initially trained on 2013 data, using 2013 gas leaks as the target variable to properly weight model parameters. They were then cross-validated by predicting 2014 gas leaks using 2013 features, with the assumption that a true predictive model would have only the previous years' data to predict the next year's leaks. In this manner, each model was optimized by tuning hyper-parameters (such as specific features selected with linear regression or the learning rate and number of hidden units in the neural network). Once sufficiently optimized, each model was tested by predicting 2015 gas leaks using 2014 features and compared based on overall RMSE.</div><div></div><h2 data-label="845242" class="ltx_title_subsection">Results</h2><div>At the zip code level, the naive model boasted the lowest overall RMSE, with 0.002446.</div><div></div><div>Table of Model Performance (Zip Code)</div><div><table data-toggle="context" data-target="#tableContextMenu" class="ltx_tabular ltx_tabular_fullpage"><tbody><tr><td class="ltx_framed ltx_align_center">Model</td> <td class="ltx_framed ltx_align_center">Total RMSE</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Naive</td> <td class="ltx_framed ltx_align_center">0.002446</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Random forest</td> <td class="ltx_framed ltx_align_center">0.002946</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Ridge regression</td> <td class="ltx_framed ltx_align_center">0.003661</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Linear regression (all features)</td> <td class="ltx_framed ltx_align_center">0.003663</td> </tr> <tr> <td class="ltx_framed ltx_align_center">MLP neural network</td> <td class="ltx_framed ltx_align_center">0.005156</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Recursive linear regression (select features only)</td> <td class="ltx_framed ltx_align_center"></td></tr></tbody></table></div><div>Table of Model Performance (Census Tract)</div><div><table data-toggle="context" data-target="#tableContextMenu" class="ltx_tabular ltx_tabular_fullpage"><tbody><tr><td class="ltx_framed ltx_align_center">Model</td> <td class="ltx_framed ltx_align_center">Total RMSE</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Naive</td> <td class="ltx_framed ltx_align_center">0.3173</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Random forest</td> <td class="ltx_framed ltx_align_center">0.4933</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Linear regression (all features)</td> <td class="ltx_framed ltx_align_center">0.7343</td> </tr> <tr> <td class="ltx_framed ltx_align_center">Ridge regression</td> <td class="ltx_framed ltx_align_center">0.7447</td> </tr> <tr> <td class="ltx_framed ltx_align_center">MLP neural network</td> <td class="ltx_framed ltx_align_center">0.8998</td></tr></tbody></table></div><div><b><div>Note&nbsp;</div></b></div><div></div><div></div><h1 class="ltx_title_section">Conclusion</h1><div>The conclusion should reinforce the major claims or interpretation in a way that is not mere summary. The writer should try to indicate the significance of the major claim/interpretation beyond the scope of the paper but within the parameters of the field. The writer might also present complications the study illustrates or suggest further research the study indicates is necessary.</div>